

# Tokenization of paragraphs/sentences
import nltk
nltk.download()


               
# Tokenizing sentences
sentences = nltk.sent_tokenize(paragraph)

# Tokenizing words
words = nltk.word_tokenize(paragraph)





